\documentclass[11pt]{article}
\IfFileExists{VERSION.tex}{\input{VERSION}}%
{\date{Pdf compiled \today. (Version unclear; run `make VERSION.tex'.)}}
\input{preamble}
\begin{document}

\author{Gray Calhoun\thanks{Economics Department, Iowa State
    University, Ames, IA 50011. Telephone: (515) 294-6271.  Email:
    \protect\url{gcalhoun@iastate.edu}. Web:
    \protect\url{http://gray.clhn.org}.  I would like to thank
    Helle Bunzel, Dimitris Politis, Robert Taylor, and three anonymous
    referees for their comments and feedback on earlier versions of
    this paper.}}

\title{Block bootstrap consistency\\under weak assumptions}

\maketitle

\begin{abstract}\noindent
  This paper weakens the size and moment conditions needed for typical
  block bootstrap methods (i.e.\ the Moving Blocks, Circular Blocks,
  and Stationary Bootstraps) to be valid for the sample mean of
  Near-Epoch-Dependent (\ned) functions of mixing processes; they are
  consistent under the weakest conditions that ensure the original
  \ned\ process obeys a Central Limit Theorem \citep[those
    of][\textit{Econometric Theory}]{Jon:97}.  In doing so, this paper
  extends De Jong's method of proof, a blocking argument, to hold with
  random and unequal block lengths.  This paper also proves that
  bootstrapped partial sums satisfy a Functional \clt\ under the same
  conditions.

  \noindent \allcaps{jel} Classification: C12, C15

  \noindent Keywords: Resampling, Time Series, Near Epoch Dependence,
  Functional Central Limit Theorem
\end{abstract}

\newpage
\noindent Block bootstraps, e.g.\ the Moving Blocks
(\citealp{Kun:89}, and \citealp{LiS:92}), Circular Block \citep{PoR:92}, and Stationary
Bootstraps \citep{PoR:94}, have become popular in Economics, partly
because they do not require the researcher to make parametric
assumptions about the data generating process.  They are valid under
general weak dependence and moment conditions.  Some recent papers
(\citealp{GoW:02}, and \citealp{GoJ:03}) relax the dependence and moment
conditions of the original papers to fit with the Near-Epoch-Dependence
(\ned) assumptions commonly used in econometrics.%
\footnote{\citet{GoW:02} show that these bootstrap methods can be
  applied to heterogeneous $L_{2+\delta}$-\ned\ processes of size
  $-2(r-1)/(r-2)$ on a strong mixing sequence of size
  $-r(2+\delta)/(r-2)$, where $r > 2$ and $\delta >0$, when the
  original series has uniformly bounded $3r$-moments.  \citet{GoJ:03}
  relax these conditions to $L_{2+\delta}$-\ned\ of size $-1$ and
  $r+\delta$ moments for the original series, and size
  $-(2+\delta)(r+\delta)/(r-2)$ for the underlying mixing series.
  Both papers require that the expected block length grow with $n$ and
  be $o(n^{1/2})$.  \cite{GoP:11} discuss these issues further.}%
\footnote{An array $\{X_{nt}\}$ is an $L_{\rho}$-\ned\ process on a
  mixing array $\{V_{nt}\}$ if
  \begin{equation}
    \| X_{nt} - \E(X_{nt}
    \mid V_{n,t-m},\dots,V_{n,t+m}) \|_{\rho} \leq d_{nt} v_m
  \end{equation}
  with $v_m \to 0$ as $m \to \infty$ and $\{d_{nt}\}$ an array of
  positive constants.  It is of size $-\gamma$ if $v_m = O(m^{-\gamma
    - \delta})$ for all $\delta>0$.  Dropping the index ``$n$'' gives
  the series definition.  Note that the underlying strong and uniform mixing arrays
  are not required to be stationary.} %
But these conditions are still stronger than
required for a \clt\ to hold; \citet{Jon:97} has established the \clt\
under $L_2$-\ned\ with smaller size and moment
restrictions.%
\footnote{\citet{Jon:97} proves that the \clt\ holds for averages of
  $L_2$-\ned\ processes of size $-1/2$ on a strong mixing series of
  size $-r/(r-2)$, $r > 2$ and the original series having bounded
  $r$-moments.} %
This paper shows that these block bootstrap
methods consistently estimate the distribution of the sample mean
under \citepos{Jon:97} assumptions and show that an \fclt\ holds as
well.%
\footnote{\citet{Rad:96} proves consistency for the Moving Blocks
  Bootstrap for any stationary strong mixing sequence that satisfies
  the \clt. This paper uses a similar method of proof to his, but also
  accommodates nonstationary sequences and the Stationary
  Bootstrap.} %
It also relaxes \citepos{GoW:02} and \citepos{GoJ:03} requirement that
the expected block length be $o(n^{1/2})$ to the original papers'
requirement that it be $o(n)$, although, as we will show, their
stronger assumption is necessary for the bootstrap to be consistent
under some forms of heterogeneity.

The proof exploits the conditional independence of the blocks in each
bootstrap.  Each bootstrap proceeds by drawing blocks of $M$
consecutive observations from the original time series, and then
pasting these blocks together to create the new bootstrap time series.
The Moving Blocks bootstrap does exactly that; the Circular Block
bootstrap ``wraps'' the observations, so that $(X_{n-1}, X_n, X_1,
X_2)$, for example, is a possible block of length four (letting $X_t$
denote the original time series).  The Stationary Bootstrap wraps the
observations and also draws $M$ at random for each block;
\citet{PoR:94} suggest drawing $M$ from the geometric distribution.
As the name suggests, the series produced by the Stationary Bootstrap
are strictly stationary, while those produced by the other methods are
not.  Although the Stationary Bootstrap was believed to be much less
efficient than other block bootstrap methods due to results of
\citet{Lah:99}, \citet{Nor:09} has shown that it is only slightly less
efficient than the other block-bootstrap methods discussed in this
paper, and has efficiency identical to that of the non-overlapping
block bootstrap.  Consequently, there has been renewed interest in the
Stationary Bootstrap since stationarity of the bootstrap samples can
be a useful property in theoretical research.  \cite{KrP:11} provides
a recent review of the bootstrap for time-series
processes%
\footnote{Also see the discussion papers by \citet{Dah:11},
\citet{GoP:11}, \citet{Hor:11}, \citet{JeM:11}, and
\citet{KrP:11b}.} %
and \citet{GoP:11} further discuss recent developments in
block-bootstraps.

Theorem~\ref{T1} presents the main result, asymptotic normality of
the distribution of bootstrapped sums. This paper adopts the
standard notation that $\E^{*}$, $\var^{*}$, etc.\ are the usual
operators with respect to the probability measure induced by the
bootstrap and will use explicit stochastic array notation
for precision.  Also note that all results are presented for the
scalar case but generalize immediately to random vectors.  All of the
proofs are presented in the appendix; only proofs for the Stationary
Bootstrap are presented, since proofs for the other methods are
similar and easier to construct.  All limits are taken as $n \to \infty$
unless otherwise noted and $\lVert \cdot \rVert_r$ denotes the
$L_r$-norm.

\begin{thm}\label{T1}
  Suppose the following conditions hold.
  \begin{enumerate}
  \item $X_{nt}$ is $L_2$-\ned\ of size $-1/2$ on an array
    $\{V_{nt}\}$ that is either strong mixing of size $-r/(r-2)$ or
    uniform mixing of size $-r/2(r-1)$, with $r > 2$.  The
    \ned\ magnitude indices are denoted $\{d_{nt}\}$.
  \item The array $\mu_{nt} - \bar \mu_n$ is uniformly bounded
    and $\sum_{t=1}^n (\mu_{nt} - \bar \mu_n)^2 = o(n p_n^2)$ (for
    the Stationary Bootstrap) or
    $\sum_{t=1}^n (\mu_{nt} - \bar \mu_n)^2 = o(n/M_n^2)$ (for
    the Moving or Circular Block Bootstraps),
    where $\E X_{nt} = \mu_{nt}$, $\bar{\mu}_n = n^{-1} \sum_{t=1}^n
    \mu_{nt}$, and $p_n$ and $M_n$ are further defined in item 4.
    Moreover, $\sqrt{n} \| \bar{X}_{n} - \bar\mu_n \|_2
    \to \sigma > 0$, with $\bar X_n = (1/n) \sum_{t=1}^n X_{nt}$.
  \item There exists an array of positive real numbers $\{c_{nt}\}$
    such that $(X_{nt} - \mu_{nt})/c_{nt}$ is uniformly $L_r$-bounded
    and $c_{nt}$ and $d_{nt}/c_{nt}$ are uniformly bounded in $n$
    and~$t$.
  \item $X_{nt}^{*}$ is generated by the Stationary Bootstrap with
    geometric block lengths with success probability $p_n$, $p_n = c
    n^{-a}$ and $a,c \in (0,1)$, or by the Moving or Circular Block
    bootstrap with block length $M_n$ such that $M_n \sim n^a$ for
    $a \in (0,1)$.  Let $M_{ni}$ be the block length of the $i$th
    block, $i=1,\dots,J_n$, and define $K_{n0} = 0$ and $K_{nj} =
    \sum_{i=1}^j M_{ni}$. The last block, $M_{n,J_n}$, is defined as
    $K_{n,J_n} - K_{n,J_n-1}$, so $K_{n,J_n} = n$ a.s.
  \end{enumerate}
  Then $\sigma^{*2} \to^p \sigma^2$, $\hat\sigma^{*2} \to^p \sigma^2$,
  \begin{equation}\label{eq:36}
    \sup_x \big\lvert \pr^{*}\big[\sqrt{n}(\bar X_{n}^{*} - \E^* \bar X_n^*) \leq x \big]
    - \pr \big[\sqrt{n}(\bar X_{n} - \E \bar X_n) \leq x \big] \big\rvert \to^p 0,
  \end{equation}
  and
  \begin{equation}
    \label{eq:2}
    \sup_x \big\lvert \pr^{*}\big[
    \sqrt{n}(\bar X_{n}^{*} -  \E^* \bar X_n^*) \big/ \hat\sigma_n^{*}
    \leq x \big] - \Phi(x) \big\rvert \to^p 0,
  \end{equation}
  where $\Phi$ is the \allcaps{cdf} of the Standard Normal
  distribution and
    \begin{equation}
      \label{eq:3}
      \hat{\sigma}_n^{*2} = \tfrac{1}{n} \sum_{j=1}^{J_n}
      \Big\{\sum_{t=K_{n,j-1}+1}^{K_{nj}} (X_{nt}^{*} - \bar X_{n}^{*})\Big\}^2.
    \end{equation}
\end{thm}

As mentioned earlier, these are the same size and mixing conditions
used by \citet{Jon:97}.
Note that De Jong does allow a little bit more flexibility in the
conditions on the array $\{c_{nt}\}$ \citep[see also][]{Dav:93};
essentially, he allows for a single set of blocks with the
maximal $\{c_{nt}\}$ over each block well-behaved, while this
paper requires this condition to hold for every possible partition of
blocks.  This additional restriction is required because the
Stationary Bootstrap will select the blocks randomly and is similar to
\citepos{JoD:00b} requirement for the \fclt.

The assumption on the dispersion of the individual means,
$(\mu_{nt} - \bar\mu_n)^2$, is the same as \citepos{GoW:02} and
\citepos{GoJ:03} under their additional requirement that the
expected block length be $o(\sqrt{n})$. When
the blocks are larger than $o(\sqrt{n})$, a case that \citet{GoW:02}
and \citet{GoJ:03} do not consider, the dispersion must be smaller
than they require.
To see these implications of this assumption, it is helpful to
consider a simple example: a stochastic process with a single break of
size $\Delta_n$ at time $T_n$:
\begin{equation*}
\mu_{nt} =
\begin{cases}
  \mu & \text{if } t \leq T_n \\
  \mu + \Delta_n & \text{otherwise}.
\end{cases}
\end{equation*}
A small amount of algebra gives the relationship,
\begin{equation*}
  \sum_{t=1}^n (\mu_{nt} - \bar \mu_n)^2
  = T_n (n - T_n) \Delta_n^2 / n \equiv D_n;
\end{equation*}
we have introduced $D_n$ to minimize notation in the subsequent
discussion.  For the Moving and Circular Block Bootstraps,
Theorem~\ref{T1}'s Assumption 2 holds if $D_n = o(n / M_n^2)$, and for
the same relationship must hold for the expected block length for the
Stationary Bootstrap. If $\Delta_n = \Delta_0$ and $T_n = \pi n$ for
some constants $\Delta_0$ and $\pi$ and for all $n$, then
$D_n \approx \Delta_0^2 \, c (1 - c) \, n$ and Assumption~2 fails to
hold. These values of $\Delta_n$ and $T_n$ describe a scenario where
there is a break large enough that $\pi$ could be precisely
estimated \citep[as in][]{Bai:94}, but the break is ignored and the
bootstrap is applied to the entire dataset.

To see when Theorem~\ref{T1}'s Assumption~2 is satisfied, consider two
other parametrizations of this DGP. First, let $\Delta_n = \Delta_0$
and $T_n = T_0$ for some finite $T_0$. This mimics a setting where a
researcher intends to apply the bootstrap to a post-break sample, but
mistakenly includes a small number of pre-break observations as
well. In this case, Assumption 2 holds as long as
$M_n = o(\sqrt{n})$.  Similarly, suppose
$\Delta_n = \Delta / \sqrt{n}$ and $T_n = \pi n$, which mimics a
setting with a moderate break with unknown timing.  Then
$D_n \approx \Delta c (1 - c)$ and Assumption 2 again holds as long as
$M_n = o(\sqrt{n})$. Consequently, using block sizes of length
$o(\sqrt{n})$ ensures that Theorem 1's assumptions on the dispersion
of the $\mu_{nt}$ hold when there is a moderate amount of
heterogeneity---either because the sample is chosen to lie after a
large break estimated break or because any breaks are smaller in
magnitude and, consequently, potentially difficult to detect. When the
degree of heterogeneity is very small or nonexistent, so
$\sum_t (\mu_{nt} - \bar \mu_n)^2$ grows even slower, then larger
block lengths can also satisfy this condition.

Theorem~\ref{T1} relies on a general insight about the
variance of the sample mean under the bootstrap-induced
distribution. It is well-known that a key step in proving the \clt\
for arbitrary dependent processes is demonstrating that the squared
elements converge to a positive and finite limit; i.e.\ if
$\{Z_{nj};~j=1,\dots,J_n\}$ is a representative stochastic array,
$\sum_j Z_{nj}^2 \to^p \sigma^2$ is an important necessary condition
for $\sum_j Z_{nj} \to^d N(0,\sigma^2)$.  (See Section 3.2 of
\citealp{HaH:80}, for further discussion.)  For martingale difference
arrays, each $Z_{nj}$ is one of the original random variables $X_{nt}$
(typically normalized by $1/\sqrt{n}$), but for other forms of
dependence (\ned\ or mixingale arrays, for example, as in
\citealp{Jon:97}) each $Z_{nj}$ is a contiguous block of the original
random variables,
\begin{equation*}
  Z_{nj} = \tfrac{1}{\sqrt{n}} \sum_{t=(j-1) M_n +1}^{j M_n} (X_{nt} - \mu_{nt}),
\end{equation*}
that adds up to the original summation (plus potentially a negligible
residual) so $\sum_j Z_{nj}= (1/\sqrt{n}) \sum_t (X_{nt} - \mu_{nt}) +
o_p(1)$. In \citet{Jon:97}, for example, the \clt\ for mixingale
arrays assumes that there exists a sequence of blocks such that $\sum_j
Z_{nj}^2$ converges i.p., and the \ned\ \clt\ establishes conditions
under which such a $Z_{nj}$ exists.

Our insight is that the expectation of squared blocks of the bootstrap
process can be expressed as a sequence of \emph{contiguous} blocks of
the original process, so the arguments that establish convergence of
the original squared blocks can be applied with only minor changes to
the bootstrapped blocks. Consider the Moving Blocks Bootstrap,%
\footnote{To make this presentation as simple as possible, assume for
  now that $n = M_n J_n$ exactly.} %
for example, and let
\begin{equation*}
  Z_{nj}^* = \tfrac{1}{\sqrt{n}} \sum_{t=(j-1) M_n + 1}^{j M_n} (X_{nt}^* - \bar X_n).
\end{equation*}
Then, conditional on the data, the $Z_{nj}^{*2}$ are independent can
be expected to obey an \lln, so %
$\sum_j (Z_{nj}^{*2} - \E^* Z_{nj}^{*2}) \to^p 0$ %
and the CLT for the bootstrapped array requires $\sum_j \E^*
Z_{nj}^{*2}$ to converge to a positive and finite limit.  But, since
$\E^*$ only averages over the starting point of each block, we have
\begin{align*}
  \E^* Z_{nj}^{*2} &=
  \tfrac{1}{n} \sum_{\tau = 0}^{n-M_n}
  \Big(\tfrac{1}{\sqrt{n}}
  \sum_{t= \tau + 1}^{\tau + M_n} (X_{nt} - \bar X_n) \Big)^2 \\
  &= \tfrac{1}{n} \sum_{\tau_0 = 0}^{M_n-1}
  \sum_{j=1}^{J_n}
  \Big(\tfrac{1}{\sqrt{n}}
  \sum_{t= (j - 1) M_n + \tau_0 + 1}^{j M_n + \tau_0} (X_{nt} - \bar X_n) \Big)^2
\end{align*}
after grouping blocks separated by $M_n$ periods. For each $\tau_0$,
the summation can be expected to converge in probability through the
same arguments that were used to establish the \clt\ for the original
array.%
\footnote{Some of the details of the argument will typically need to
  change because the original \clt\ only requires convergence for
  $\tau_0 = 0$, but these details are often incidental to the original
  argument.}%
\footnote{Lemmas~\ref{L4} and
  \ref{L6} are particularly
  strong demonstrations of this argument.} %
A similar representation is available for the Circular and Stationary
bootstraps.

In short, the basic approach that we use to prove Theorem~\ref{T1}
is based on a fundamental connection between the second moments of the
bootstrap process and the sum of squared blocks of the original
array. Even though the details of our proof rely on specific
techniques for \ned\ arrays, this connection implies that block
bootstraps are typically consistent when the original dependent array
obeys the \clt\ and the connection should be useful for proving
consistency of the bootstrap under other dependence conditions.

Theorem~\ref{T1} can also be extended to give an \fclt\
using arguments from \citet{JoD:00b}. We show in
Theorem~\ref{T2} that the partial sum
of the bootstrapped process obeys an \fclt\ and can be used to derive
critical values for other test statistics under the same assumptions
as Theorem~\ref{T1}. For this result, define the
following partial sums,
\begin{align*}
W_n(\gamma) &=
\tfrac{1}{\sqrt{n}} \sum_{t=1}^{\lfloor \gamma n \rfloor}
(X_{nt} - \bar\mu_n) &&\text{and}&
W_n^{*}(\gamma) &= \tfrac{1}{\sqrt{n}}
\sum_{t=1}^{\lfloor \gamma n \rfloor}
(X^*_{nt} - \E^* \bar X_n^*).
\end{align*}
Also let $W$ denote standard Brownian Motion and $\sigma W$ denote
Brownian Motion scaled by the constant $\sigma$.

\begin{thm}\label{T2}
Suppose that the conditions of Theorem~\ref{T1} hold
and let $\dist$ be any distance function that metricizes weak
convergence. Then
\begin{equation}\label{eq:7}
\pr^{*}[\dist(W_n^{*}, \sigma W) > \delta] \to^p 0
\end{equation}
for any positive  $\delta$.
If, in addition, $\sup_{t=1,\dots,n}|\mu_{nt} -
\bar \mu_n| = o(1/\sqrt{n})$ and
\begin{equation}\label{eq:4}
n^{-1} \sum_{s,t=1}^{\lfloor \gamma n \rfloor} \cov(X_{ns}, X_{nt}) \to \sigma^2 \gamma
\end{equation}
for all $\gamma \in [0,1]$, then
\begin{equation}\label{eq:8}
\pr[\dist(W_n, \sigma W) > \delta] \to 0
\end{equation}
for any positive $\delta$.
\end{thm}
If both~\eqref{eq:7} and~\eqref{eq:8} hold then the bootstrap can be
used to approximate the distribution of partial sums.
Note that Theorem~\ref{T2} imposes
stronger assumptions for the original process
than for the bootstrapped process. Without~\eqref{eq:4}, the
partial sum of the original observations converges to a transformed
Brownian Motion with a different covariance process. The bootstrapped
partial sum, on the other hand, always converges to standard (but
potentially scaled) Brownian Motion because the resampling strategies
ensure that the bootstrapped process is globally covariance stationary.

If the original process does not satisfy~\eqref{eq:4}, it would be
necessary to normalize $W_n$ with a uniformly
consistent estimator of the true covariance process of the series to
make use of these results. This would be the case if the variance
permanently changes partway through the series, for example.  Other
methods, such as the Local Block Bootstrap \citep{PaP:02,DPP:03}, may
be able to capture this additional heterogeneity with the bootstrap
alone, but we do not pursue that possibility further.

The rest of the paper presents the mathematical proofs in detail.

\appendix
\section{Proof of main results}

For both results, we only present a proof for the stationary
bootstrap. The moving blocks and circular block bootstrap follow the
same general argument but are simpler. We will define some additional
notation here before presenting the proofs.

First, the relevant probability infrastructure.  Let
$(S, \Omega, \pr)$ be a probability space and define the sequence of
sub-sigma-fields $\Omega_n \subset \Omega$. Assume that each
$X_{nt}$, $V_{nt}$, $M_{nj}$, and $u_{nj}$ is $\Omega_n$-measurable,
with $u_{nj}$ the $\operatorname{uniform}(1,\dots,n)$ random variables
that designate the start period of each bootstrap ``block.'' Also
define the \sigmafield\ generated by the stationary bootstrap's block
lengths alone,
\begin{equation}
  \Ms = \sigma(J_{n}, M_{n1},\dots,M_{nJ_n})
\end{equation}
and the conditional probability $\Pm(\cdot) = \pr[\cdot \mid \Ms]$.
(And define $\Pm^*(\cdot) = \pr^*[\cdot \mid \Ms]$, $\Em(\cdot) =
\E^*(\cdot \mid \Ms)$, etc.) An important property is
that $\Ms$ is \emph{independent} of the $X_{nt}$'s, $V_{nt}$'s, and
$u_{nj}$'s, so we can treat any
$M_{nj}$ and $J_n$ terms as constants within $\Em(\cdot)$ and
$\Pm[\cdot]$ and integrate over the unconditional distributions of the
other random variables. This property is especially important because it allows us
to use maximal inequalities and other moment inequalities for
mixingale arrays with only small modifications to construct almost sure bounds on the
conditional moments $\Em$.

Also define
\begin{equation}
  \label{eq:10}
  I_n(\tau, m) = \begin{cases}
    \{\tau + 1,\dots, \tau + m\} &
    \textif\ 0 \leq \tau \leq n - m \text{\ and\ } 1 \leq m \\
    \{1,\dots, m - n + \tau\} \cup \{\tau + 1,\dots, n\} &
    \textif\ n - m < \tau \leq n \text{\ and\ } 1 \leq m\\
    \emptyset & \textif\  m \leq 0,
  \end{cases}
\end{equation}
so each $I_n(\tau, m)$ defines a potential block of length $m$ of the
original observations that could be chosen by the bootstrap.%
\footnote{The index sets $I_n(\tau, m)$ are designed to ``wrap
  around'' and use the first observations when $\tau + m > n$,
  matching the defining aspect of the stationary and circular block
  bootstraps.} %
By convention, summations over empty index sets will be considered
equal to zero. Note that the $I_n$ satisfy
\begin{align}\label{eq:39}
   \begin{split}
   I_n(K_{n0}, M_{n1}) &= \{1,\dots,K_{n1}\} \\
   I_n(K_{n1}, M_{n2}) &= \{K_{n1}+1,\dots,K_{n2}\} \\
   \vdots \\
   I_n(K_{n,J_n-1}, M_{nJ_{n}}) &= \{K_{n,J_{n-1}}+1,\dots,n \},
   \end{split}
\end{align}
so $I_n(K_{n0}, M_{n1}),\dots,I_n(K_{n,J_n-1}, M_{nJ_{n}})$ exactly
partition the set $\{1,\dots,n\}$ into consecutive blocks with
lengths determined by the bootstrap.

Let
\newcommand{\Zb}[1][j]{{Z^*_{n{#1}}}}
\begin{align}\label{eq:11}
  Z_n(\tau, m) &= \tfrac{1}{\sqrt{n}} \sum_{t\in I_n(\tau, m)} (X_{nt} - \bar X_n) \\
  Z_n^*(\tau,m)&= \tfrac{1}{\sqrt{n}} \sum_{t \in I_n(\tau,m)} (X_{nt}^{*} - \bar X_n) \label{eq:12}
\intertext{and}
  \Zb          &= Z_n^*(K_{n,j-1}, M_{nj}) = \tfrac{1}{\sqrt{n}} \sum_{t=K_{n,j-1}+1}^{K_{n,j}} (X_{nt}^* - \bar X_n) \label{eq:1}
\intertext{and define the corresponding demeaned terms}
  Z_n'(\tau, m) &= \tfrac{1}{\sqrt{n}} \sum_{t\in I_n(\tau, m)} (X_{nt} - \mu_{nt})
  \label{eq:13}\\
  Z_n^{\prime*}(\tau,m)&= \tfrac{1}{\sqrt{n}} \sum_{t \in I_n(\tau,m)} (X_{nt}^{*} - \mu_{nt}^*)
  \label{eq:14}
\intertext{and}
  Z_{nj}^{\prime *} &= Z_n^{\prime*}(K_{n,j-1}, M_{nj}), \label{eq:15}
\end{align}
where $\mu_{nt}^*$ is the expected value of the observation in the
original dataset corresponding to the $t$th observation in the
bootstrapped dataset. Further, define the filtration
\begin{equation}
\Gs_{nj} = \sigma(Z^*_{n1},\dots, Z^*_{nj},~X_{n1},\dots,X_{nn},~\Ms)
\end{equation}
so that $\{Z_{nj}^* / \sigma_n^*, \Gs_{nj}\}$ is a martingale difference
array.

By construction, (see Equations~\eqref{eq:39} and~\eqref{eq:1})
\begin{equation}\label{eq:16}
  \tfrac{1}{\sqrt{n}} \sum_{t=1}^n (X_{nt}^{*} - \bar X_n)
  = \sum_{j=1}^{J_n} \Zb
\end{equation}
and
\begin{equation}\label{eq:17}
  \Em^{*}g(\Zb,\dots,\Zb[k]) = \frac{1}{n^{k-j+1}}
  \sum_{\tau_1=0}^{n-1} \cdots \sum_{\tau_{k-j+1}=0}^{n-1}
  g(Z_n(\tau_1, M_{nj}),\dots,Z_n(\tau_{k-j+1}, M_{nk}))
\end{equation}
almost surely for any function $g$ and any $j \leq k$. Equation~\eqref{eq:17}
conditions on the lengths of each block, but averages over their
starting points.

\subsection*{Proof of Theorem~\ref{T1}}
{%
\newcommand{\Db}[1][j]{D_{n#1}^{*}}
\newcommand{\Zsum}{\sum_{j=1}^{J_n} \Zb / \sigma_n}

First we prove that
\begin{equation}\label{eq:37}
  \sup_x \big\lvert \pr^{*}\big[
  \sqrt{n}(\bar X_{n}^{*} -  \bar X_n) \big/ \sigma_n^{*}
  \leq x \big] - \Phi(x) \big\rvert \to^p 0
\end{equation}
where $\sigma_n^{*2} = n \E^* (\bar X_n^* - \bar X_n)^2$.
Rewrite $\sqrt{n}(\bar X_{n}^{*} - \bar X_n)$ as in
Equation~\eqref{eq:16}, so
\begin{equation*}
  \tfrac{1}{\sqrt{n}} \sum_{t=1}^n (X_{nt}^* - \bar X_n) =
  \sum_{j=1}^{J_n} Z_{nj}^*
\end{equation*}
and $\{\Zb / \sigma_n^*, \Gs_{nj}\}$ is a martingale difference
array. Moreover,
\begin{equation}
  \label{eq:18}
  \Pm^{*} \Big[\sum_{j=1}^{J_n} \Zb / \sigma_n^* \leq x \Big] - \Phi(x) \to^p 0,
\end{equation}
for all $x$ if $\sigma_n^{*2} \to^p \sigma^2$ (which ensures that
$\sigma_n^{*2}$ is uniformly a.s.\ positive and holds by
Lemma~\ref{L4}) and the following
two conditions hold for all positive $\epsilon$:
\begin{equation}
  \label{eq:19}
  \sum_{j=1}^{J_n} \Em^{*} \big(Z_{nj}^{*2} \ind\{Z_{nj}^{*2}  >
  \epsilon\}\big) \to^p 0
\end{equation}
and
\begin{equation}
  \label{eq:20}
  \Pm^{*}\Big[\ \Big|\sum_{j=1}^{J_n} Z_{nj}^{*2} - \sigma_n^{*2}
  \Big|\ > \epsilon \Big] \to^p 0
\end{equation}
since~\eqref{eq:19} and~\eqref{eq:20} ensure that $\Zb/\sigma_n^*$ obeys
a martingale difference \clt\ \citep[e.g.][Theorem 3.3]{HaH:80}.%
\footnote{Conditional on $X_{n1},\dots,X_{nn}$, $J_n$, and
  $M_{n1},\dots,M_{n,J_n}$, the only stochastic components of $\Zsum$
  are the start periods of each block, which are
  $\discreteuniform(1,\dots,n)$ and are independent of all of the
  other random variables in the information set used for conditioning.
  Consequently, $\Pm^*$ is a regular conditional probability and
  arguments like Hall and Heyde's (1980) Theorem 3.3
  apply without modification on this probability measure. See
  also Section 23.2 of \citet{Vaa:00}.
  Also note that Hall and Heyde's Theorem 3.3 as stated imposes an
  additional restriction on the sigma-fields. However, as Hall and
  Heyde discuss on pages 59 and 63--64, that condition is unnecessary
  here because $\sigma_n^{*2}$ is measurable with respect to
  all of the $\Gs_{nj}$.} %

For~\eqref{eq:20}, $Z_{nj}^*$ and $Z_{nk}^*$ (when $k \neq j$) are
conditionally uncorrelated given $X_{n1},\dots,X_{nn},$ and $\Ms$, which
implies
\begin{equation*}
  \sum_{j=1}^{J_n} Z_{nj}^{*2} - \sigma_n^{*2} =
  \sum_{j=1}^{J_n} \Big( Z_{nj}^{*2} - (1/J_n) \E^* \sum_{j=1}^{J_n} Z_{nj}^{*2} \Big)
\end{equation*}
almost surely. But
\begin{equation*}
   \Big\{\tfrac{n}{M_{nj}} \Big(Z_{nj}^{*2} - (1/J_n) \E^* \sum_{j=1}^{J_n} Z_{nj}^{*2}\Big),
   \ \Gs_{nj} \Big\}
\end{equation*}
is a uniformly-integrable martingale difference array
by Lemma~\ref{L6}
and satisfies the \lln\ (e.g., \citealp{Dav:94}, Theorem 19.7). So
this sum converges to zero in conditional probability, proving~\eqref{eq:20}.

To prove~\eqref{eq:19}, it suffices to show that
\begin{equation}\label{eq:21}
  \Em \sum_{j=1}^{J_n} \Em^{*}\big(Z_{nj}^{*2} \ind\{Z_{nj}^{*2} > \epsilon\}\big) \to^p 0.
\end{equation}
For any $j$ and $m$ and for large enough $n$,
\begin{align}
\Em \Em^{*}\big((Z_n(u_{nj}, m)^2n/m) &\ind\{Z_n(u_{nj}, m)^2n/m > \epsilon n/m\}\big) \notag \\
&= \E \big((Z_n^{*}(0, m)^2n/m) \ind\{Z_n^{*}(0, m)^2n/m > \epsilon n/m\}\big) \notag \\
&\leq B(\epsilon n/m)\label{eq:5}
\end{align}
where $B$ is some finite monotone function such that $B(x) \to 0$ as
$x \to \infty$; the existence of a function $B$ that satisfies these
conditions (and does not depend on $n$ or $m$) is a consequence of
Lemma~\ref{L6}. Since $J_n$ and $M_{nj}$ are both $\Ms$-measurable
and independent of the $u_{nj}$'s and $X_{nt}$'s,~\eqref{eq:5} implies that
\begin{align*}
\Em &\sum_{j=1}^{J_n} \Em^{*}\big(Z_{nj}^{*2} \ind\{Z_{nj}^{*2} > \epsilon\}\big) \\
& = \sum_{j=1}^{J_n} (M_{nj}/n) \Em \Em^{*}
\big((Z_n(u_{nj}, M_{nj})^2 n/M_{nj})
\ind\{Z_n(u_{nj}, M_{nj})^2n/M_{nj} > \epsilon n/M_{nj}\}\big) \\
& \leq \sum_{j=1}^{J_n} (M_{nj}/n) \, B(\epsilon n/M_{nj}) \\
& \leq \max_{j = 1,\dots,J_n} B(\epsilon n / M_{nj}) \sum_{i=1}^{J_n} M_{ni} / n \\
& = B(\epsilon n / \max_{j = 1,\dots,J_n} M_{nj}) \\
& \to^p 0 \text{\ as\ } n \to \infty.
\end{align*}
The first equality follows from simple algebra and the definition
of the $u_{nj}$'s. The first inequality is a
consequence of~\eqref{eq:5}.  The second equality holds by
monotonicity of $B$. And convergence in probability holds by
Lemma~\ref{L1}, completing the proof
of~\eqref{eq:19}. The Dominated Convergence Theorem and~(\ref{eq:18})
then ensure that
\begin{equation}\label{eq:22}
  \pr^{*} \Big[\sum_{j=1}^{J_n}
  Z_{nj}^{*} / \sigma_n^* \leq x \Big] - \Phi(x) \to^p 0.
\end{equation}
(Also see
Lemma~\ref{L2}.)

Lemma~\ref{L4} implies that
$\sigma_n^{*2}$ and $\hat\sigma_n^{*2}$ both converge to $\sigma^2$ in
probability. This convergence then implies that
\begin{equation}
  \label{eq:23}
  \pr^{*}\big[\sqrt{n}(\bar X_{n}^{*} - \bar X_n) \leq x\big] \to^p \Phi(x/\sigma)
\end{equation}
and
\begin{equation}
\label{eq:24}
  \pr^{*}\big[\sqrt{n}(\bar X_{n}^{*} - \bar X_n) / \hat\sigma_n^{*}
  \leq x\big] \to^p \Phi(x)
\end{equation}
for any $x$.  These results are sufficient for~\eqref{eq:36}
and~\eqref{eq:2} though an argument attributed to Poly{\`a}
that proceeds as follows.
Let $k$ be a finite integer and define $x_i = \sigma \Phi^{-1}(i/k)$ for $i =
0,\dots,k$ (so $x_0 = -\infty$ and $x_k = +\infty$).
For any $x \in [x_i, x_{i+1}]$,
\begin{align*}
  \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x] - \Phi(x/\sigma)
  &\leq \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x_{i+1}] - \Phi(x_i/\sigma) \\
  &= \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x_{i+1}] - \Phi(x_{i+1}/\sigma) + 1/k
\intertext{and}
  \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x] - \Phi(x/\sigma)
  &\geq \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x_i] - \Phi(x_{i+1}/\sigma) \\
  &= \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x_i] - \Phi(x_i/\sigma) - 1/k
\end{align*}
almost surely. Then
\begin{multline*}
  \sup_{x \in (-\infty, +\infty)} \big| \pr^*\big[\sqrt{n} (\bar X_n^* - \bar X_n) \leq x] - \Phi(x/\sigma) \big| \\
  \leq \sup_{i=0,\dots,k} \big|\pr^{*}\big[\sqrt{n}(\bar X_{n}^{*} - \bar X_n) \leq x_i \big] - \Phi(x_i/\sigma) \big| + 1/k
\end{multline*}
almost surely and~\eqref{eq:23} ensures that
\begin{equation*}
  \sup_{i=0,\dots,k} \big|\pr^{*}\big[\sqrt{n}(\bar X_{n}^{*} - \bar X_n)
  \leq x_i \big] - \Phi(x_i/\sigma) \big| + 1/k \to^p 1/k
\end{equation*}
for any finite $k$. Since $k$ is arbitrary,~\eqref{eq:37} holds. Since
Theorem~\ref{T1}'s assumptions ensure that the original array obeys
the \clt,~\eqref{eq:36} holds \citep[Theorem 2]{Jon:97}.  A similar
argument applies to the asymptotic distribution of
$\sqrt{n}(\bar X_n^* - \bar X_n) / \hat\sigma_n^*$, completing the
proof.\qed }

\subsection*{Proof of Theorem~\ref{T2}}
\newcommand{\gn}{{\lfloor \gamma n \rfloor}}
\newcommand{\gJ}{{\lfloor \gamma J_n \rfloor}}
\newcommand{\lastK}{{K_{n, \gJ}}}

We will only present proofs for the bootstrap results, since
Theorem~3.1 of \cite{JoD:00b} establishes the result for the partial
sums of the original process.  Theorem~\ref{T1}
implies that, for any fixed $\gamma$, $W_n^{*}(\gamma)$ is
asymptotically normal with limiting variance $\gamma \sigma^2$, so we
can assume that $\sigma^2 = 1$ without loss of generality. Moreover,
Lemma~\ref{L2} shows that it is
sufficient to prove unconditional convergence, so we will establish
$\pr^*[\dist(W_n^{*}, W) > \delta)] \to^p 0$. As in \cite{JoD:00b},
this will hold if we show that $W_n^{*}$ has asymptotically
independent increments and stochastic equicontinuity, namely
\begin{equation}\label{eq:9}
  \lim_{\delta \to 0} \limsup_{n \to \infty} \pr[\sup_{\gamma \in [0,1]}
  \sup_{\gamma' \in [\gamma - \delta, \gamma + \delta]}
    | W_n^{*}(\gamma) - W_n^{*}(\gamma') | > \epsilon] = 0
\end{equation}
for any positive $\epsilon$.

First observe that we can write $W_n^{*}$ as
\begin{equation*}
  W^{*}_n(\gamma)
  = \tfrac{1}{\sqrt{n}} \sum_{t=1}^\gn (X_{nt}^* - \bar X_{n})
  = \sum_{j=1}^\gJ \Zb + Z_n^*(\lastK, \gn - \lastK).
\end{equation*}
To show that the increments of this process are asymptotically
independent, choose $\gamma, \gamma' \in [0,1]$ and
$\delta, \delta' > 0$ so that $\delta + \gamma \leq \gamma'$. Since
the blocks of $W_n^{*}$ are conditionally uncorrelated given
$X_{1n},\dots,X_{nn}$, and $\Ms$, we have
\begin{equation*}
  \E\big[ (W_n^{*}(\delta' + \gamma') - W_n^{*}(\gamma'))
  (W_n^{*}(\delta + \gamma) - W_n^{*}(\gamma)) \big] = 0
\end{equation*}
for large enough $n$ if $\gamma' > \gamma + \delta$. If $\gamma' =
\gamma + \delta$ then
\begin{multline*}
  \E\big[(W_n^{*}(\delta' + \gamma') - W_n^{*}(\gamma'))
  (W_n^{*}(\delta + \gamma) - W_n^{*}(\gamma)) \big] = \\
  \E \Em^*\big\{- Z_n^*(\lfloor \gamma' n \rfloor, K_{n, \lceil \gamma' J_n \rceil }- \lfloor \gamma' n \rfloor)
  Z_n^*(K_{n,\lfloor (\gamma + \delta) J_n \rfloor},
  \lfloor (\gamma + \delta) n \rfloor - K_{n,\lfloor (\gamma + \delta) J_n \rfloor}))\big\}.
\end{multline*}
But this second quantity can be bounded:
\begin{align*}
  \E \Em^*\big\{-& Z_n^*(\lfloor \gamma' n \rfloor,
  K_{n, \lceil \gamma' J_n \rceil }- \lfloor \gamma' n \rfloor)
  Z_n^*(K_{n,\lfloor (\gamma + \delta) J_n \rfloor},
  \lfloor (\gamma + \delta) n \rfloor - K_{n,\lfloor (\gamma + \delta) J_n \rfloor}))\big\} \\
  &\leq \big\{\E \Em\big[Z_n^*(\lfloor \gamma' n \rfloor, K_{n, \lceil \gamma' J_n \rceil }- \lfloor \gamma' n \rfloor)\big]^2 \\
  & \quad \times \big\{\E \Em Z_n^*(K_{n,\lfloor (\gamma + \delta) J_n \rfloor},
  \lfloor (\gamma + \delta) n \rfloor - K_{n,\lfloor (\gamma + \delta) J_n \rfloor}))^2\big\}^{1/2} \\
  &\leq C \E M_{n, \lfloor (\gamma + \delta) J_n \rfloor} / n
\end{align*}
for some constant $C$ by
Lemma~\ref{L6}. This term
converges to zero by Lemma~\ref{L1}.

For~\eqref{eq:9}, fix $\delta> 0$ such that $D = 2/\delta$ is a
positive integer and let $\gamma_d = d/D$ for $d =
0,\dots,D$. Mimicking the argument in \cite{JoD:00b} gives the bounds
\begin{align*}
  \pr[\sup_{\gamma \in [0,1]} &
  \sup_{\gamma' \in [\gamma - \delta, \gamma + \delta]}
  | W_n^{*}(\gamma) - W_n^{*}(\gamma') | > \epsilon] \\
  &\leq \pr[ \sup_{d = 1,\dots,D} \sup_{\gamma \in [0,\delta]}
  | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) | > \epsilon/2] \\
  &\leq  (4/\epsilon^2) \sum_{d=1}^D \E[\sup_{\gamma \in [0,\delta]}
  | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 \\
  &\qquad\qquad\qquad \times \ind\{\sup_{\gamma \in [0,\delta]}
  | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 > \epsilon^2/4\}] \\
  &\leq  (4/\epsilon^2) \max_{d=1,\dots,D} \E[\sup_{\gamma \in [0,\delta]}
  (1/\delta) | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 \\
  &\qquad\qquad\qquad \times \ind\{\sup_{\gamma \in [0,\delta]}
  (1/\delta) | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 > \epsilon^2/4\delta\}].
\end{align*}
Lemma~\ref{L6} implies that
\begin{equation*}
  \sup_{\gamma \in [0,\delta]}
  (1/\delta) | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2
\end{equation*}
is uniformly integrable, so an argument similar to that used in the
proof of Theorem~\ref{T1} shows that there exists a
finite and monotone function $B$ such that $B(x) \to 0$ as
$x \to \infty$ and
\begin{multline*}
  \E[\sup_{\gamma \in [0,\delta]}
  (1/\delta) | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 \times \\
  \ind\{\sup_{\gamma \in [0,\delta]}
  (1/\delta) | W_n^{*}(\gamma + \gamma_d) - W_n^{*}(\gamma_d) |^2 > x\}]
  \leq B(x)
\end{multline*}
for all $d$ and $\delta$ and all large enough $n$.

As a result,
\begin{align*}
  \lim_{\delta \to 0} \limsup_{n\to\infty} \pr[\sup_{\gamma \in [0,1]}
  \sup_{\gamma' \in [\gamma - \delta, \gamma + \delta]}
  | W_n^{*}(\gamma) - W_n^{*}(\gamma') | > \epsilon] & \leq
  \lim_{\delta \to 0} (4/\epsilon^2)
  B(\epsilon^2/4\delta) \\
  &= 0,
\end{align*}
completing the proof.\qed

\section{Supplemental results}

\begin{lem}\label{L1}
  Suppose that $M_{n1}, M_{n2},\dots$ are i.i.d.\ geometric random
  variables with success parameter $p_n = c n^{-a}$ with $a, c \in
  (0,1)$, and that $\ell_n = (p_n \log p_n^{-1})^{-1}$ and define
  $J_n$ so that $\sum_{i=1}^{J_n-1} M_{ni} < n \leq \sum_{i=1}^{J_n} M_{ni} $
  Then
  \begin{enumerate}
  \item $\max_{i=1,\dots,\lfloor C n p_n \rfloor} M_{ni} / n \to^p 0$
    for any positive $C$,
  \item $\max_{i=1,\dots,J_n} M_{ni} / n \to^p 0$,
  \item $\max_{i=1,\dots,  \lfloor C n p_n \rfloor} M_{ni} /
  \ell_n^{1+\epsilon} \to^p 0$ as $n \to \infty$ for any positive
  $\epsilon$ and $C$,
  \item $\max_{i=1,\dots,J_n} M_{ni} /
  \ell_n^{1+\epsilon} \to^p 0$ as $n \to \infty$ for any positive
  $\epsilon$, and
  \item $\E\big(\sum_{i=1}^{J_n} M_{ni}^2\big) = (2 n - 1) / p_n - n + 1$.
  \end{enumerate}
\end{lem}

\begin{proof}[Proof of Lemma~\ref{L1}]
  To prove part 1, observe that, for any increasing positive sequence
  $x_n$ such that $x_n p_n \to \infty$,
  \begin{equation*}
    \pr\Big[\max_{i=1,\dots, \lfloor C n p_n \rfloor} M_{ni} \leq x_n\Big] =
    (1 - (1 - p_n)^{x_n})^{\lfloor C n p_n \rfloor} \to \lim \exp(-C n p_n (1 - p_n)^{x_n})
  \end{equation*}
  and $C n p_n (1 - p_n)^{x_n} \to \lim C n p_n e^{-x_n p_n}$. Now, let
  $x_n = n x$ for any positive number $x$.  Then
  \begin{equation*}
    \pr\Big[\max_{i=1,\dots,\lfloor C n p_n \rfloor} M_{ni}/n \leq x \Big] \to
    \lim \exp(-C n p_n e^{-n p_n x}) =
    \exp(0) = 1.
  \end{equation*}
  Since $x$ is arbitrary, $\max_{i=1,\dots,\lfloor C n p_n \rfloor} M_{ni} / n \to^p 0$.

  For part 2, take $C$ to be an arbitrary constant strictly greater
  than one. For any $x$,
  \begin{align*}
    \pr\Big[\max_{i=1,\dots,J_n} M_{ni} > x\Big] & \leq
    \pr\Big[\max_{i=1,\dots,\lfloor C n p_n \rfloor} M_{ni} > x
    \text{\ or\ } J_n > \lfloor C n p_n \rfloor\Big] \\
    & \leq \pr\Big[\max_{i=1,\dots,\lfloor C n p_n \rfloor} M_{ni} > x\Big]
    + \pr\Big[\sum\nolimits_{i=1}^{\lfloor C n
      p_n \rfloor} M_{ni} < n \Big]
  \end{align*}
  The first term converges to zero by part 1 and the second term by
  the \lln.

  For part 3, let $x_n = \ell_n^{1 + \epsilon} x$ and note that
  \begin{equation*}
    p_n \ell_n^{1+\epsilon} \geq p_n^{-(\epsilon-\delta-\epsilon\delta)} =
    c^{-(\epsilon-\delta-\epsilon\delta)}
    n^{a(\epsilon-\delta-\epsilon\delta)} \equiv b n^{a(\epsilon
      - \delta - \epsilon\delta)}
  \end{equation*}
  for any $\delta > 0$ and large enough $n$.  Choose $\delta$ small
  enough that $\epsilon > \delta(1 +\epsilon)$. Then
  \begin{equation*}
    n p_n \exp(-\ell_n^{1+\epsilon} p_n) \leq n p_n
    \exp(-b n^{a(\epsilon -
      \delta - \epsilon\delta)}) = c
    v_n^{\frac{1-a}{a(\epsilon-\delta-\epsilon\delta)}}
    \exp(-b v_n) \to 0,
  \end{equation*}
  with $v_n = n^{a(\epsilon-\delta-\epsilon\delta)}$.  Consequently,
  \begin{equation*}
    \pr[\max_i M_{ni}/\ell_n^{1+\epsilon} \leq x ] \to \exp(0) = 1
  \end{equation*}
  as well.
  The proof of part 4 is the same as part 2, making the obvious
  substitutions.

  For part 5, write
  \begin{equation}\label{eq:41}
    \sum_{i=1}^{J_n} M_{ni}^2 =
    \sum_{i=1}^{J_n} (M_{ni} - \tfrac{1}{p_n})^2
    + \tfrac{2}{p_n} \sum_{i=1}^{J_n} (M_{ni} - \tfrac{1}{p_n})
    + \tfrac{J_n}{p_n^2}
  \end{equation}
  so
  \begin{equation*}
    \E \Big( \sum_{i=1}^{J_n} M_{ni}^2 \Big)
    = \E\Big( \sum_{i=1}^{J_n} (M_{ni} - \tfrac{1}{p_n})^2 \Big)
    + \tfrac{2 n}{p_n} - \E \tfrac{J_n}{p_n^2}.
  \end{equation*}
  Since $J_n$ is a stopping time and $\sum_{i=1}^j (M_{ni} - 1/p_n)$
  is a martingale, we have the equality \citep[Lemma 6]{CRT:65}
  \begin{align*}
    \E\Big( \sum_{i=1}^{J_n} (M_{ni} - \tfrac{1}{p_n})^2 \Big)
    &= \E\Big( \sum_{i=1}^{J_n} \E((M_{ni} - \tfrac{1}{p_n})^2 \mid M_{n1},\dots,M_{n,i-1})\Big)\\
    &= \tfrac{1 - p_n}{p_n^2} \E J_n.
  \end{align*}
  Finally, $\E J_n = (n-1) p_n + 1$, completing the proof.
\end{proof}

\begin{lem}\label{L2}
  If $\{A_n\}$ is a sequence of events in $\Omega$ then the following
  are equivalent:
  \begin{align*}
  \pr[A_n] &\to 0, &
  \pr^*[A_n] &\to 0 \text{\ in\ }L_1, &
  &\text{and}&
  \Pm^*[A_n] &\to 0 \text{\ in\ }L_1.
  \end{align*}
\end{lem}
\begin{proof}
  Since $|\pr[A_n]| = \E |\pr^*[A_n]| = \E |\pr^*_{\mathcal{M}}[A_n]|$ these
  conditions are equivalent by definition.
\end{proof}

\begin{lem}\label{L3}
Under the conditions of Theorem~\ref{T1},
\begin{equation}\label{eq:40}
  \E \Big(\sum_{j=1}^{J_n} \Big( \sum_{t = K_{n,j-1}+1}^{K_{nj}}
  (\mu_{nt}^{*} - \bar \mu_n)\Big)^2\Big) = o(n p_n)
\end{equation}
and $\bar \mu_n^{*} = \E^{*} \bar \mu_n^{*} + o_{L_2}(n^{-1/2})$.
\end{lem}
\begin{proof}
  The second result, on $\bar\mu_n^*$, is an immediate implication of
  Equation~\eqref{eq:40}. To show~\eqref{eq:40}, observe that
  \begin{align*}
    \E \Big(\sum_{j=1}^{J_n} &\Big( \sum_{t= K_{n,j-1} +1}^{K_{nj}}
    (\mu_{nt}^{*} - \bar \mu_n)\Big)^2\Big)= \\
    &= \E\Big(\sum_{j=1}^{J_n} \big\lvert \sum_{s,t = K_{n,j-1}+1}^{K_{nj}}
    \E \big((\mu_{ns}^{*} - \bar{\mu}_n)
    (\mu_{nt}^{*} - \bar{\mu}_n) \mid \Ms \big) \big\rvert \Big)\\
    &\leq \E\Big( \sum_{j=1}^{J_n}
    \sum_{k,l = -n}^{n} \tfrac{1}{n} \sum_{\tau=0}^{n-1} \big\lvert
    (\mu_{n,\tau+k} - \bar{\mu}_n) (\mu_{n,\tau+l} - \bar{\mu}_n) \;
    1\{ \tau + k, \tau + l \in I_n(\tau, M_{nj})\} \big\rvert \Big)\\
    &\leq \E\Big(\sum_{j=1}^{J_n} \Big[
    \sum_{k = -n}^{n} \big(\tfrac{1}{n} \sum_{\tau=0}^{n-1}
    (\mu_{n,\tau+k} - \bar{\mu}_n)^2
    1\{ \tau + k \in I_n(\tau, M_{nj})\}\big)^{1/2}\Big]^{2}\Big)\\
    &= \E\Big(\sum_{j=1}^{J_n}
    M_{nj}^2 \Big\lvert \tfrac{1}{n} \sum_{\tau=0}^{n-1}
    (\mu_{n,\tau} - \bar{\mu}_n)^2 \Big\lvert\Big)\\
    &= o(n p_n).
  \end{align*}
  The first equality follows from the LIE. The first inequality
  exploits the fact that the conditional expectation in the previous
  expression averages over the block's starting period but treats
  every other random variable as contstant. (The argument of the
  indicator function and the indices of the summations are
  deliberately chosen to accomodate blocks that include observations
  from both the beginning and the end of the series.) The second
  inequality is an application of the Cauchy-Schwarz inequality to the
  inner-most summation. The next equality holds because the
  summation
  \[
    \sum_{\tau=0}^{n-1} (\mu_{n,\tau+k} - \bar{\mu}_n)^2
    1\{ \tau + k \in I_n(\tau, M_{nj})\}
  \]
  is zero for all but $M_{nj}$ values of $k$ and is a nonzero constant
  for those $k$. The last line holds by Lemma~\ref{L1} and assumption.
\end{proof}

\begin{lem}\label{L4}
Under the conditions of Theorem~\ref{T1},
\begin{gather}
  \label{eq:25}
  \sum_{j=1}^{J_n} (Z_{nj}^{*2} - \Em^* Z_{nj}^{*2}) \to^p 0,
  \\
  \label{eq:26}
  \sum_{j=1}^{J_n} (Z_{nj}^{*2} - Z_{nj}^{\prime*2}) \to^p 0,
\end{gather}
and $\pr^*[ |\sigma_n^{*2} - \sigma^2| > \epsilon] \to^p 0$.
If, in addition, $\bar X_n^* - \bar X_n = O_p(1/\sqrt{n})$ then
$\pr^*[ |\hat\sigma_n^{*2} - \sigma^2| > \epsilon] \to^p 0$.
\end{lem}

\begin{proof}
  For~\eqref{eq:25}, $(Z_{nj}^{*2} - \Em^* Z_{nj}^{*2}) \cdot (n /
  M_{nj})$ is a uniformly integrable martingale difference array, by
  Lemma~\ref{L6}, and
  satisfies the \lln. (See~\citealp{Dav:94}, Theorem 19.7.)
  For~\eqref{eq:26}, observe that
  \begin{align*}
    \Big| \sum_{j=1}^{J_n} (Z_{nj}^{*2} - Z_{nj}^{\prime*2}) \Big|
    &= \Big| \sum_{j=1}^{J_n} \big(Z_{nj}^{*} - \big[Z_{nj}^{*} + \tfrac{1}{\sqrt{n}} \sum_{t=K_{n,j-1}+1}^{K_{nj}} (\bar X_n - \mu_{nt}^*)\big]^2\big) \Big| \\
    &\leq 2 \Big(\sum_{j=1}^{J_n} Z_{nj}^{*2} \Big)^{1/2}
    \Big(\tfrac{1}{n} \sum_{j=1}^{J_n} \Big( \sum_{t= K_{n,j-1} + 1}^{K_{nj}}
    (\mu_{nt}^{*} - \bar X_n)\Big)^2 \Big)^{1/2}\\
    &\quad + \tfrac{1}{n} \sum_{j=1}^{J_n} \Big( \sum_{t= K_{n,j-1} + 1}^{K_{nj}}
    (\mu_{nt}^{*} - \bar X_n)\Big)^2
  \end{align*}
  from the Cauchy-Schwarz inequality. Lemma~\ref{L5}, along
  with~\eqref{eq:17} and~\eqref{eq:25}, implies that
  $\sum_j Z_{nj}^{*2} = O_p(1)$. Lemma~\ref{L3} implies that
  \[
    \tfrac{1}{n} \sum_{j=1}^{J_n} \Big( \sum_{t= K_{n,j-1} + 1}^{K_{nj}}
    (\mu_{nt}^{*} - \bar X_n)\Big)^2 \to^p 0
  \]
  since $\bar X$ itself obeys the \lln.

  To show that $\sigma_n^{*2}$ converges, we can write
  \begin{multline}\label{eq:38}
    \sigma_n^{*2} - \sigma^2 = \E^*\Big\{ \sum_{j=1}^{J_n} (Z_{nj}^{*2} - Z_{nj}^{\prime*2}) \Big\}
    + \tfrac{1}{n} \sum_{\tau=0}^{n-1} \E^* \Big\{\sum_{j=1}^{J_n}
    \big( Z_{n}^{\prime}(\tau,M_{nj})^2 - \Em Z_{n}^{\prime}(\tau,M_{nj})^2 \big)\Big\} \\
    + \tfrac{1}{n} \sum_{\tau=0}^{n-1} \E^* \Big\{\sum_{j=1}^{J_n}
    \Em Z_{n}^{\prime}(\tau,M_{nj})^2  - \sigma^2\Big\}.
  \end{multline}
  Uniform integrability ensures that the convergence in~(\ref{eq:25})
  holds in $L_1$ as well and Lemma~\ref{L2} then implies that
  the first term in \eqref{eq:38} converges to zero in probability.
  Lemma~\ref{L5} proves that the second and third
  summation converge to zero in probability.

  Next,
  \begin{equation*}
    \hat \sigma_n^{*2} - \sigma_n^{*2}
    = \sum_{j=1}^{J_n} \big( Z_{nj}^* + (M_{nj}/\sqrt{n}) (\bar X_n - \bar X_{n}^*) \big)^2
    - \E^*  \sum_{j=1}^{J_n} Z_{nj}^{*2}
  \end{equation*}
  so, in light of the previous arguments, $\hat\sigma_n^{*2} \to^p \sigma^2$ if
  \begin{equation}
    (\bar X_n - \bar X_n^*)^2 \sum_{j=1}^{J_n} M_{nj}^2/n \to^p 0,
  \end{equation}
  which holds by Lemma~\ref{L1} and
  assumption.
\end{proof}

\begin{lem}\label{L5}
  If the conditions of Theorem~\ref{T1} hold then
  \begin{gather}
    \pr\Big[ \Big\lvert
    \tfrac{1}{n} \sum_{\tau=0}^{n-1} \sum_{j=1}^{J_n}
    \big[ Z_n'(\tau, M_{nj})^2 - \Em Z_n'(\tau, M_{nj})^2 \big]
    \Big\rvert > \epsilon \Big] \to 0 \label{eq:27}
    \intertext{and}
    \pr\Big[ \Big\lvert
    \tfrac{1}{n} \sum_{\tau=0}^{n-1} \sum_{j=1}^{J_n}
    \Em Z_n'(\tau, M_{nj})^2 - \sigma^2
    \Big\rvert > \epsilon \Big] \to 0.\label{eq:28}
  \end{gather}
\end{lem}
\newcommand{\nMj}{\lfloor n / M_{nj} \rfloor}

\noindent For these two proofs, let $\ell_n = (p_n
\log p_n^{-1})^{-1}$ and let $L_{nj} = \nMj$; $\ell_n$ represents a
smaller block size that satisfies $\ell_n J_n / n \to^p 0$.

\begin{proof}[Proof of~(\ref{eq:27})]
  We can express this summation as
  \begin{multline}\label{eq:29}
    \tfrac{1}{n} \sum_{\tau=0}^{n-1}  \sum_{j=1}^{J_n}
    \big\{Z_n'(\tau, M_{nj})^2 - \Em Z_n'(\tau, M_{nj})^2 \big\}
    \\ =
    \tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1}
    \sum_{i=0}^{L_{nj}-1}\big\{\big[ Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n) + Z_n'(\tau + (i+1) M_{nj} - \ell_n, \ell_n)\big]^2 \\
    - \Em\big[ Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n) + Z_n'(\tau + (i+1) M_{nj} - \ell_n, \ell_n)\big]^2 \big\} \\
    + \tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau= M_{nj}L_{nj}}^{n-1}
    \big\{Z_{n}'(\tau, M_{nj})^2 - \Em Z_n'(\tau, M_{nj})^2 \big\}
  \end{multline}
  almost surely.
  By Lemma~\ref{L7} (Equation~\ref{eq:32}), for
  any $\delta > 0$ there exist positive $C$ and $\epsilon$ such that
  \begin{align*}
    \Big\lVert\tfrac{1}{n} \sum_{j=1}^{J_n} & \sum_{\tau=0}^{M_{nj}-1}
    \sum_{i=0}^{L_{nj}-1} \big\{ Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n)^2 -
    \Em \big(Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n)^2\big) \big\}\Big\rVert_1 \\
    &\leq \E \tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1}
    \Em \Big| \sum_{i=0}^{L_{nj}-1} \big\{ Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n)^2 -
    \Em \big(Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n)^2\big) \big\} \Big|\\
    &\leq \E \Big(\tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1}
    \big(2 \delta + C \, \tfrac{M_{nj}}{n^{1/2} \ell^{1/2+\epsilon}}\big)\Big)
  \end{align*}
  for large enough $n$, which converges to $2 \delta$ by
  Lemma~\ref{L1}.
  Lemma~\ref{L8} ensures that there
  exists a value $C$ (possibly different from the value above) such
  that
  \begin{align*}
    \E\Big(\tfrac{1}{n}  \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1} \sum_{i=0}^{L_{nj}-1}
    & Z_n'(\tau + (i+1) M_{nj} - \ell_n, \ell_n)^2 \Big) \\
    &= \E \Em \Big(\tfrac{1}{n}  \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1} \sum_{i=0}^{L_{nj}-1}
    Z_n'(\tau + (i+1) M_{nj} - \ell_n, \ell_n)^2 \Big) \\
    &\leq C \E \Big(\sum_{j=1}^{J_n} L_{nj} \ell_n M_{nj} / n^2\Big)
  \end{align*}
  and
  \begin{equation*}
    \E \Big(\tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau= M_{nj}L_{nj}}^{n-1} Z_{n}'(\tau, M_{nj})^2\Big)
    \leq
    C \E \Big(\sum_{j=1}^{J_n} M_{nj}^2 / n^2 \Big)
  \end{equation*}
  for large enough $n$,%
\footnote{Lemma~\ref{L7}
  and~\ref{L8} are stated for the
  unconditional expectation, which may cause some confusion here. Note
  that $M_{nj}$ is measurable with respect to $\mathcal{M}_n$ and
  is treated the same as the constant $m$ in the statement of
  these Lemmas.  The other random variables in
  these expressions are independent of $\mathcal{M}_n$. So the only
  effect of conditioning on $\mathcal{M}_n$ is to prevent integration
  over the distributions of the $M_{nj}$ and the expectation otherwise
  behaves exactly like the unconditional expectation.\label{f:1}} %
  both of which converge to zero in $L_1$ as $n \to \infty$ by
  Lemma~\ref{L1}.  These three convergence
  results imply that the \allcaps{rhs} of (\ref{eq:29}) converges to
  zero in probability, completing the proof.
\end{proof}

\begin{proof}[Proof of~(\ref{eq:28})]
  After using similar arguments to the previous part of the proof,
  the conclusion holds if
  \begin{equation*}
    \tfrac{1}{n} \sum_{j=1}^{J_n} \sum_{\tau=0}^{M_{nj}-1} \sum_{i=0}^{L_{nj} - 1}
    \Em Z_n'(\tau + i M_{nj}, M_{nj} - \ell_n)^2 \to^p \sigma^2,
  \end{equation*}
  which is a direct implication of
  Lemma~\ref{L7}.%
  \footnote{The discussion in Footnote~\ref{f:1} applies here as well.}
\end{proof}

\begin{lem}\label{L6}
  Under the conditions of Theorem~\ref{T1},
  \begin{multline}\label{eq:30}
    \lim_{C \to \infty} \limsup_{n \to \infty} \sup_{\substack{\tau = 0,\dots,n-1 \\ m'=1,\dots,n}}
    \E\Big(\max_{m = 1,\dots,m'} (n \, Z^*_n(\tau, m)^2 / m') \\
    \times \ind\Big\{\max_{m = 1,\dots,m'} n \, Z^*_n(\tau, m)^2 / m' > C\Big\}\Big)
    = 0.
  \end{multline}
  Moreover, the families of random variables
  $\{\max_{m=1,\dots,m'} Z^{*}_n(\tau, m)^2 n / m'; \tau, m', n\}$ and
  $\{\max_{m=1,\dots,M_{nj}} Z^{*}_{nj}(u_{nj}, m)^2 n / M_{nj}; j, n\}$ are uniformly integrable.
\end{lem}

\begin{proof}
  We will only present a proof of~\eqref{eq:30}. Uniform integrability
  of the first family of random variables is simply a restatement of
  that expression and uniform integrability of $Z_{nj}^{*2} n / M_{nj}$
  follows an abbreviated version of the proof of~\eqref{eq:30}.

  First we will show that, for every $\epsilon > 0$ and large enough
  $n$, there exists an $\epsilon' > 0$ with the property that
  \begin{equation}\label{eq:31}
    \E \Big(\ind(A) \times \max_{m = 1,\dots,m'} Z^*_n(\tau, m)^2 n / m'\Big) < \epsilon
  \end{equation}
  for any event $A \in \Fs_n \equiv \sigma(X_{n1},\dots,X_{nn})$ with
  $\pr(A) \leq \epsilon'$. Then we will show that this property
  implies uniform integrability.\footnote{%
    The novelty in this part of the proof is the measurability
    requirement. The sigma-field $\Fs_n$ intentionally excludes the
    random variables that define the bootstrap (block length and block
    start periods) and any $A \in \Fs_n$ is independent of those
    variables, so $Z_n^{*}(\tau, m)^2$ is not $\Fs_n$-measurable. If
    $\Fs_n$ were enlarged to include those variables then uniform
    integrability would be an immediate consequence of~\eqref{eq:31}
    and would not need to be shown separately. See \citet[Theorem
    12.9]{Dav:94}.} %

  Take an arbitrary $\delta > 0$ and a value of $\delta' > 0$
  so that
  \begin{equation}\label{eq:35}
    \sup_{m'=1,\dots,n} \E \Big(\ind(A) \max_{m = 1,\dots,m'}
    Z_n'(\tau, m)^2 n / m' \Big) < \delta
  \end{equation}
  for any $A \in \Fs_n$ with $\pr(A) \leq \delta'$; the existence
  of $\delta'$ is ensured by Lemma~\ref{L8}. For any $x$,
  define $J(x)$ to be the block index such that $K_{n,J(x) - 1} < x
  \leq K_{n,J(x)}$. For any $m$, we can decompose $Z_n^{*}(\tau, m)$
  into a summation of conditionally independent blocks
  \begin{equation*}
  Z^{*}_n(\tau, m) = Z_n^*(\tau, K_{n,J(\tau)} - \tau) +
  \sum_{j=J(\tau)+1}^{J(\tau + m)-1} Z_{nj}^* + Z_n^*(K_{n,J(\tau+m)-1}, m - K_{n,J(\tau + m)-1}),
  \end{equation*}
  giving
  \begin{align}
  \E  \Big(&\ind(A) \times \max_{m = 1,\dots,m'} n \, Z^*_n(\tau, m)^2 / m'\Big)
  \notag \\
  &\leq 3 \E\Big[ \Em^{*} \Big[\ind(A) \tfrac{n}{m'}\max_{m = 1,\dots, m'}
  \Big(Z_n^*(\tau, K_{n,J(\tau)} - \tau) + \sum_{j=J(\tau)+1}^{J(\tau + m)-1} Z_{nj}^*\Big)^2\Big]
  \notag \\
  &\quad + 3 \Em^{*} \big[\ind(A) \tfrac{n}{m'}
  \max_{m=1,\dots,m'} Z_n^*(K_{n,J(\tau+m)-1}, m - K_{n,J(\tau + m)-1})^2 \big]\Big]\label{eq:6}
  \end{align}
  almost surely.

  Since $Z_{nj}^*$ is a martingale difference array, Doob's maximal
  inequality for martingales \citep[see][Theorem 15.15, for
  example]{Dav:94} along with the construction of $Z_{nj}^{*}$ gives
  a bound for the first term on the \allcaps{RHS} of~\eqref{eq:6},
  \begin{align*}
  \E \Em^* & \Big[\ind(A) \tfrac{n}{m'}\max_{m = 1,\dots, m'}
  \Big(Z_n^*(\tau, K_{n,J(\tau)} - \tau)
  + \sum_{j=J(\tau)+1}^{J(\tau + m)-1} Z_{nj}^*\Big)^2\Big] \\
  &\leq \tfrac{1}{m'} \sum_{u = 0}^{n-1} \E \Big[\Em \big[
  \ind(A) Z_n(u, K_{n,J(\tau)} - \tau)^2\big] +
  \sum_{j=J(\tau)+1}^{J(\tau + m')-1} \Em \big[\ind(A) Z_n(u, M_{nj})^2]\Big] \\
  &\leq \tfrac{3}{m'} \sum_{u = 0}^{n-1} \E\Big[\Em\big[
  \ind(A) Z'_n(u, K_{n,J(\tau)} - \tau)^2\big]
  + \tfrac{1}{n} \Pm[A] \Big(\sum_{t \in I_n(u, K_{n,J(\tau)} - \tau)}
  (\mu_{nt} - \bar \mu_n)\Big)^2 \\
  & \quad+ \sum_{j=J(\tau)+1}^{J(\tau + m')-1}
  \Em \big[\ind(A) Z_n'(u, M_{nj})^2]
  + \tfrac{1}{n} \Pm[A]
  \sum_{j=J(\tau)+1}^{J(\tau + m')-1}\Big( \sum_{t \in I_n(u,M_{nj})}
  (\mu_{nt} - \bar{\mu}_n) \Big)^2 \\
  & \quad+ \Em\big( \ind(A) Z_n'(u, n)^2 \big)
  \sum_{j=J(\tau)}^{J(\tau+m')-1} M_{nj}^2/n^2 \Big] \\
  &\leq (3\delta/m') \E (K_{n,J(\tau+m')-1} - \tau)
    + 3\delta \E\Big(\sum_{j=J(\tau)}^{J(\tau+m')-1} M_{nj}^2/n^2 \Big) \\
  &\quad+ \tfrac{3\delta'}{nm'} \sum_{u=0}^{n-1}\E\Big( \Big(\sum_{t \in I_n(u, K_{n,J(\tau)} - \tau)}
  (\mu_{nt} - \bar \mu_n)\Big)^2 +
  \sum_{j=J(\tau)+1}^{J(\tau + m')-1}\Big( \sum_{t \in I_n(u,M_{nj})}
  (\mu_{nt} - \bar{\mu}_n) \Big)^2 \Big) \\
  & < C_1 \delta + C_2 p_n \delta'
  \end{align*}
  for some finite constants $C_1$ and $C_2$.
  The second inequality comes from writing
  \begin{align*}
    Z_n(\tau, m) &= Z_n'(\tau, m)
    + \tfrac{1}{\sqrt{n}} \sum_{t \in I_n(\tau, m)}
    \big((\mu_{nt} - \bar \mu_n) + (\bar \mu_n - \bar X_n)\big) \\
    &= Z_n'(\tau, m)
    + \tfrac{1}{\sqrt{n}} \sum_{t \in I_n(\tau, m)} (\mu_{nt} - \bar \mu_n)
    - (m/n) Z'_n(\tau, n),
  \end{align*}
  the third follows by construction of $A$
  and the fourth follows from an argument similar to Lemma~\ref{L3}.

  For the second term on the \allcaps{RHS} of~\eqref{eq:6}, use the
  sequence of inequalities
  \begin{align*}
  \E \Em^* \big[\ind(A) &\tfrac{n}{m'}
  \max_{m=1,\dots,m'} Z_n^*(K_{n,J(\tau+m)-1}, m - K_{n,J(\tau + m)-1})^2 \big] \\
  &\leq \E \Big(\sum_{j=J(\tau) + 1}^{J(\tau+m')-1} \tfrac{n}{m'} \Em^* \big[\ind(A)
  \max_{m=1,\dots, \min(M_{nj}, m' - K_{n,j-1})} Z_n^*(K_{n,j}, m)^2 \big]\Big) \\
  &\leq \E \Big(\sum_{j=J(\tau) + 1}^{J(\tau+m')} \tfrac{1}{m'} \sum_{u=0}^{n-1} \Em \big[\ind(A)
  \max_{m=1,\dots,\min(M_{nj}, m' - K_{n,j-1})} Z_n(u, m)^2 \big]\Big) \\
  &\leq \E \Big(\sum_{j=J(\tau) + 1}^{J(\tau+m')} \tfrac{3}{m'} \sum_{u=0}^{n-1} \Big\{
  \Em \big[\ind(A)
  \max_{m=1,\dots,\min(M_{nj}, m' - K_{n,j-1})} Z_n'(u, m)^2 \big] \\
  &\quad +
  \tfrac{1}{n} \Pm[A] \max_{m = 1,\dots,\min(M_{nj}, m'-K_{n,j-1})}
  \Big(\sum_{t \in I_n(u,m)} (\mu_{nt} - \bar{\mu}_n)\Big)^2 \\
  &\quad+ \tfrac{\min(M_{nj},m'-K_{n,j-1})^2}{n^2} \Em( \ind(A) Z_n'(u, n)^2 ) \Big\}\Big)\\
  &\leq C'_1 \delta + C_2' \delta' p_n
  \end{align*}
  for constants $C_1'$ and $C_2'$. To express these bounds in the form
  of~\eqref{eq:31}, let $\epsilon > 0$ be arbitrary, choose $\delta'$
  such that~\eqref{eq:35} holds for $\delta = \epsilon/(2C_1+2C_1')$,
  and let $\epsilon' = \min(\delta', \epsilon/(2 C_2 + 2C_2'))$.
  A similar argument also
  implies that $n Z_n^*(\tau, m)^2 / m'$ has finite first moment.

  Now, for uniform integrability, take $\epsilon > 0$, choose
  $\epsilon'$ so that~\eqref{eq:31} holds for all $A \in \Fs_n$
  s.t. $\pr(A) \leq \epsilon'$, and choose $C_0$ large enough that
  \begin{equation*}
    \Pr\Big[ \max_{m = 1,\dots,m'} n \, Z^*_n(\tau, m)^2 / m' > C_0 \Big] \leq \epsilon'.
  \end{equation*}
  Markov's inequality guarantees the existence of this $C_0$. Then
  \begin{equation*}
    \E \Big(\max_{m = 1,\dots,m'} n \, Z^*_n(\tau, m)^2 / m'
    \times \ind\Big\{ \max_{m = 1,\dots,m'} n \, Z^*_n(\tau, m)^2 / m' > C \Big\}\Big) \leq \epsilon
  \end{equation*}
  for all $C > C_0$. Since $\epsilon$ is arbitrary, this completes the proof.
\end{proof}

\begin{lem}\label{L7}
\newcommand{\isum}{\sum_{i=0}^{\lfloor n/m \rfloor - 1}}
  Suppose the conditions of Theorem~\ref{T1} hold.
  For any positive $\delta$, there exist positive and finite constants
  $C$, $n_0$, and $\epsilon$ such that
  for all $n > n_0$, $m = 1,\dots,n$, $\tau = 0,\dots,m$, and $\ell =
  1,\dots,m-1$:
\begin{multline}\label{eq:32}
    \E \Big\lvert \isum \big[Z_n'(\tau + i m, m - \ell)^2
    - \E \big(Z_n'(\tau + i m, m - \ell)^2\big) \big]\Big\rvert \\
    \leq 2 \delta + C \cdot \big(\tfrac{m}{n}\big)^{1/2}
    \big(\tfrac{m}{\ell^{1+\epsilon}}\big)^{1/2}.
\end{multline}
Also, there exists a constant $C$ and a finite function $D(x)$ such
that $D(x) \to 0$ as $x \to \infty$ and, for large enough $n$,
  \begin{equation}
    \label{eq:33}
    \E \Big\lvert \isum \E(Z_n'(\tau + i m, m - \ell)^2
    - \sigma^2 \Big\rvert \leq C\, D(\ell).
  \end{equation}
\end{lem}

Results~\eqref{eq:32} and~\eqref{eq:33} are direct extensions of
\citepos{Jon:97} Lemmas 5 and 4, respectively, replacing De Jong's
implicit use of inequalities with explicit inequalities. The
supplemental appendix presents a proof of~\eqref{eq:33} to show the
main idea.

Note that these results apply immediately to the conditional expectation
$\Em$ because the block lengths $M_{jn}$ do not appear in~\eqref{eq:32}
and~\eqref{eq:33} and are independent of all of the random variables
used for these bounds.

\begin{lem}\label{L8}
\newcommand{\uiterm}{\max_{m \in 1,\dots,m'} \Big(
  \sum_{t \in I_n(\tau, m)} (X_{nt} - \mu_{nt})\Big)^2 \Big/
  \sum_{t\in I_n(\tau, m')} c_{nt}^2}
\newcommand{\uitermb}{(1/m') \max_{m \in 1,\dots,m'} \Big(
  \sum_{t \in I_n^*(\tau, m)} (X_{nt} - \mu_{nt})\Big)^2}
  Under the conditions of Theorem~1,
  \begin{multline}\label{eq:34}
    \lim_{C \to 0} \limsup_{n \to \infty} \sup_{\substack{\tau = 0,\dots,n-1 \\  m' = 1,\dots,n}}
    \E\big(\big(\max_{m=1,\dots,m'} Z'_n(\tau, m)^2 n / m'\big) \\
    \times\ind\{\max_{m=1,\dots,m'} Z'_n(\tau, m)^2 n / m' > C\}\big)) = 0.
  \end{multline}
\end{lem}

For proof, see supplemental appendix for the proof of~\eqref{eq:34}, which
follows \citet[Lemma 6.5]{Mcl:75b} and \citet[Lemma 3.5]{Mcl:77}
almost exactly and is also presented as Theorem~16.13 in
\citet{Dav:94}. The same comment that follows
Lemma~\ref{L7} applies here as well: the bounds
apply equally well to $\Em$.

\bibliography{texextra/references}
\end{document}

% LocalWords:  CLT Kun LiS PoR GoW GoJ nt indices eq studentized JoD reindex nj
% LocalWords:  De Jong's ns Mcl AllRefs nn th nm ni HaH formulae jn gcalhoun nN
% LocalWords:  jel PaP DPP np Ames Resampling nonstationary cdf Helle

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
